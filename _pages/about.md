---
layout: about
title: about
permalink: /
subtitle: Shanghai Jiao Tong University. (zengzhi.wang [at] sjtu dot edu dot cn). 

profile:
  align: right
  image: sentosa_1.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Shanghai, China Photos</p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder
  size: 7

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

Hi, there! I am Zengzhi Wang (王增志), a first-year PhD student at [GAIR Lab](https://plms.ai/), Shanghai Jiao Tong University, advised by [Prof. Pengfei Liu](http://pfliu.com/). Before that, I received my master's degree in Computer Science at the Nanjing University of Science & Technology advised by Prof. Rui Xia and Assoc. Prof. Jianfei Yu. I obtained my bachelor's degree in Software Engineering at Wuhan Institute of Technology.

I curated data and trained models — and in turn, data, models, and results also trained me. Recently, I focus on

- Curating pre-training corpora, i.e., [MathPile](https://github.com/GAIR-NLP/MathPile) (9.5B tokens, [NeurIPS 2024](https://openreview.net/pdf?id=RSvhU69sbG)) and [MegaMath](https://github.com/LLM360/MegaMath) (370B tokens, [Preprint](https://arxiv.org/abs/2504.02807)).
- Refining pre-training corpora at scale, i.e., [ProX](https://github.com/GAIR-NLP/ProX) (refining corpora by tiny language models at scale, [ICML 2025](https://arxiv.org/abs/2409.17115v2)) along with refined byproducts, such as [FineWeb-Pro (100B tokens)](https://huggingface.co/datasets/gair-prox/FineWeb-pro) and [DCLM-Pro (>500B tokens)](https://huggingface.co/datasets/gair-prox/DCLM-pro). Check [Huggingface](https://huggingface.co/gair-prox) for more releases.
- Enhancing the capabilities of foundation models by mid-training and RL scaling, i.e., [OctoThinker](https://tinyurl.com/OctoThinker).
- Other interesting topics that are currently exploring.